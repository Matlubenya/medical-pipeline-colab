{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# ğŸ¥ Medical Data Pipeline\n",
    "\n",
    "**Complete pipeline for processing Ritter PDFs and STATIM TXT files**\n",
    "\n",
    "## ğŸš€ Quick Start\n",
    "1. **Run all cells** (Runtime â†’ Run all)\n",
    "2. **Upload 5 Python scripts** when prompted\n",
    "3. **Upload 2 PDF + 2 TXT files** when prompted  \n",
    "4. **Pipeline runs automatically**\n",
    "\n",
    "## ğŸ“ Required Files\n",
    "### Python Scripts:\n",
    "- `ritter_pdf_to_txt.py`\n",
    "- `ritter_txt_to_csv.py`\n",
    "- `statim_parser.py`\n",
    "- `merged2.py`\n",
    "- `report2.py`\n",
    "\n",
    "### Data Files:\n",
    "- 2 Ritter PDF files\n",
    "- 2 STATIM TXT files\n",
    "\n",
    "**GitHub Repository**: https://github.com/Matlubenya/medical-pipeline-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 1. SETUP & DOWNLOAD FROM GITHUB\n",
    "# =======================\n",
    "print(\"ğŸš€ Medical Data Pipeline - FRESH VERSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Running in Google Colab\")\n",
    "    \n",
    "    # Create fresh workspace\n",
    "    workspace = Path('/content/medical_pipeline')\n",
    "    if workspace.exists():\n",
    "        shutil.rmtree(workspace)\n",
    "        print(\"ğŸ§¹ Cleaned old workspace\")\n",
    "    \n",
    "    workspace.mkdir()\n",
    "    os.chdir(workspace)\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    workspace = Path.cwd()\n",
    "    print(\"ğŸ’» Running locally\")\n",
    "\n",
    "print(f\"ğŸ“ Workspace: {workspace}\")\n",
    "\n",
    "# DOWNLOAD ESSENTIAL SCRIPTS FROM GITHUB\n",
    "if IN_COLAB:\n",
    "    print(\"\\nğŸ“¥ Downloading setup scripts from GitHub...\")\n",
    "    \n",
    "    import urllib.request\n",
    "    \n",
    "    github_repo = \"Matlubenya/medical-pipeline-colab\"\n",
    "    scripts = [\"setup_colab.py\", \"run_pipeline.py\", \"requirements.txt\"]\n",
    "    \n",
    "    for script in scripts:\n",
    "        try:\n",
    "            url = f\"https://raw.githubusercontent.com/{github_repo}/main/{script}\"\n",
    "            destination = workspace / script\n",
    "            urllib.request.urlretrieve(url, destination)\n",
    "            print(f\"  âœ… {script}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸  {script}: {e}\")\n",
    "            # Create minimal version if download fails\n",
    "            if script == \"setup_colab.py\":\n",
    "                minimal = '''import os\\nfrom pathlib import Path\\n\\ndef main():\\n    print(\\\"Setting up directory structure...\\\")\\n    workspace = Path.cwd()\\n    dirs = [\\\"scripts/parsing_codes\\\", \\\"scripts/statistics_codes\\\", \\\"data/raw/pdfs\\\", \\\"data/raw/txts\\\", \\\"output\\\"]\\n    for d in dirs:\\n        (workspace / d).mkdir(parents=True, exist_ok=True)\\n    print(\\\"âœ… Setup complete\\\")\\n    return workspace\\n\\nif __name__ == \\\"__main__\\\":\\n    main()'''\n",
    "                destination.write_text(minimal)\n",
    "            elif script == \"run_pipeline.py\":\n",
    "                minimal = '''import subprocess\\nfrom pathlib import Path\\n\\ndef main():\\n    print(\\\"Running pipeline scripts...\\\")\\n    scripts = [\\n        (\\\"scripts/parsing_codes/statim_parser.py\\\", \\\"STATIM Parser\\\"),\\n        (\\\"scripts/parsing_codes/ritter_pdf_to_txt.py\\\", \\\"Ritter PDF Parser\\\"),\\n        (\\\"scripts/parsing_codes/ritter_txt_to_csv.py\\\", \\\"Ritter CSV Converter\\\"),\\n        (\\\"scripts/statistics_codes/merged2.py\\\", \\\"Statistical Analysis\\\"),\\n        (\\\"scripts/statistics_codes/report2.py\\\", \\\"Report Generator\\\")\\n    ]\\n    \\n    for script_path, description in scripts:\\n        full_path = Path(script_path)\\n        if full_path.exists():\\n            print(f\\\"\\\\nğŸš€ {description}: {full_path.name}\\\")\\n            subprocess.run([\\\"python\\\", str(full_path)])\\n        else:\\n            print(f\\\"âš ï¸  Missing: {script_path}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()'''\n",
    "                destination.write_text(minimal)\n",
    "    \n",
    "    print(\"\\nâœ… Setup scripts downloaded\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 2. INSTALL DEPENDENCIES\n",
    "# =======================\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Installing dependencies...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Install from requirements.txt if available\n",
    "    req_file = workspace / \"requirements.txt\"\n",
    "    if req_file.exists():\n",
    "        print(\"Installing from requirements.txt...\")\n",
    "        !pip install -q -r requirements.txt\n",
    "    else:\n",
    "        # Install essential packages\n",
    "        packages = [\"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"scipy\", \"reportlab\", \"tqdm\"]\n",
    "        for pkg in packages:\n",
    "            !pip install -q {pkg}\n",
    "    \n",
    "    # Install pdftotext\n",
    "    print(\"Installing pdftotext (for PDF parsing)...\")\n",
    "    !apt-get update -qq && apt-get install -y poppler-utils 2>/dev/null\n",
    "    \n",
    "    print(\"\\nâœ… Dependencies installed\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"ğŸ’» Using local Python environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 3. SETUP ENVIRONMENT\n",
    "# =======================\n",
    "print(\"ğŸ”§ Setting up environment...\")\n",
    "\n",
    "# Run setup script\n",
    "setup_script = workspace / \"setup_colab.py\"\n",
    "if setup_script.exists():\n",
    "    print(\"Running setup_colab.py...\")\n",
    "    !python setup_colab.py\n",
    "else:\n",
    "    print(\"âš ï¸ setup_colab.py not found. Creating basic structure...\")\n",
    "    !mkdir -p scripts/parsing_codes scripts/statistics_codes data/raw/pdfs data/raw/txts output\n",
    "\n",
    "print(\"\\nğŸ“ Directory structure:\")\n",
    "!find . -maxdepth 3 -type d | sort\n",
    "\n",
    "print(\"\\nâœ… Environment ready\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-scripts"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 4. UPLOAD PIPELINE SCRIPTS\n",
    "# =======================\n",
    "print(\"ğŸ“ Upload Your 5 Pipeline Scripts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"\\nğŸ“¤ Please upload these 5 Python files:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    scripts_needed = [\n",
    "        (\"ritter_pdf_to_txt.py\", \"parsing_codes\"),\n",
    "        (\"ritter_txt_to_csv.py\", \"parsing_codes\"),\n",
    "        (\"statim_parser.py\", \"parsing_codes\"),\n",
    "        (\"merged2.py\", \"statistics_codes\"),\n",
    "        (\"report2.py\", \"statistics_codes\")\n",
    "    ]\n",
    "    \n",
    "    for script, folder in scripts_needed:\n",
    "        print(f\"  â€¢ {script} â†’ scripts/{folder}/\")\n",
    "    \n",
    "    print(\"\\nâ„¹ï¸  Use the file browser on the left\")\n",
    "    print(\"   Upload one by one or all at once\")\n",
    "    \n",
    "    # Wait for upload\n",
    "    input(\"\\nğŸ“ Press Enter AFTER uploading all 5 scripts...\")\n",
    "    \n",
    "    # Organize uploaded files\n",
    "    import glob\n",
    "    uploaded = glob.glob(\"*.py\")\n",
    "    \n",
    "    if uploaded:\n",
    "        print(f\"\\nğŸ“¦ Organizing {len(uploaded)} uploaded files...\")\n",
    "        \n",
    "        for script_path in uploaded:\n",
    "            script_name = os.path.basename(script_path)\n",
    "            target_folder = None\n",
    "            \n",
    "            # Find correct folder\n",
    "            for needed_script, folder in scripts_needed:\n",
    "                if script_name == needed_script:\n",
    "                    target_folder = folder\n",
    "                    break\n",
    "            \n",
    "            if target_folder:\n",
    "                target_path = f\"scripts/{target_folder}/{script_name}\"\n",
    "                os.rename(script_path, target_path)\n",
    "                size_kb = os.path.getsize(target_path) / 1024\n",
    "                print(f\"  âœ… {script_name} â†’ scripts/{target_folder}/ ({size_kb:.1f} KB)\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  {script_name} (not a required script)\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No Python files uploaded\")\n",
    "        print(\"   You can upload later using file browser\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nğŸ’» Local mode: Ensure scripts are in scripts/parsing_codes/ and scripts/statistics_codes/\")\n",
    "\n",
    "# Verify scripts\n",
    "print(\"\\nğŸ“‹ Script Status:\")\n",
    "missing = []\n",
    "for script, folder in scripts_needed:\n",
    "    script_path = workspace / \"scripts\" / folder / script\n",
    "    if script_path.exists():\n",
    "        print(f\"  âœ… {script}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {script} (MISSING)\")\n",
    "        missing.append(script)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nâš ï¸  Missing {len(missing)} scripts\")\n",
    "    print(\"   Upload them using the file browser\")\n",
    "else:\n",
    "    print(\"\\nğŸ‰ All pipeline scripts ready!\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-data"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 5. UPLOAD DATA FILES\n",
    "# =======================\n",
    "print(\"ğŸ“¤ Upload Data Files\")\n,
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"\\nğŸ“„ Upload 2 Ritter PDF files:\")\n",
    "    print(\"Files will be saved to: data/raw/pdfs/\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    pdf_files = []\n",
    "    \n",
    "    for filename, content in uploaded.items():\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            dest = workspace / \"data\" / \"raw\" / \"pdfs\" / filename\n",
    "            dest.write_bytes(content)\n",
    "            pdf_files.append(filename)\n",
    "            size_mb = len(content) / (1024 * 1024)\n",
    "            print(f\"  âœ… {filename} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Upload 2 STATIM TXT files:\")\n",
    "    print(\"Files will be saved to: data/raw/txts/\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    txt_files = []\n",
    "    \n",
    "    for filename, content in uploaded.items():\n",
    "        if filename.lower().endswith('.txt'):\n",
    "            dest = workspace / \"data\" / \"raw\" / \"txts\" / filename\n",
    "            dest.write_bytes(content)\n",
    "            txt_files.append(filename)\n",
    "            size_kb = len(content) / 1024\n",
    "            print(f\"  âœ… {filename} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Upload Summary:\")\n",
    "    print(f\"  â€¢ PDF files: {len(pdf_files)} ({', '.join(pdf_files[:3])}{'...' if len(pdf_files) > 3 else ''})\")\n",
    "    print(f\"  â€¢ TXT files: {len(txt_files)} ({', '.join(txt_files[:3])}{'...' if len(txt_files) > 3 else ''})\")\n",
    "    \n",
    "    if len(pdf_files) < 1 or len(txt_files) < 1:\n",
    "        print(\"\\nâš ï¸  Need at least 1 PDF and 1 TXT file\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nğŸ’» Local mode: Ensure data files are in data/raw/pdfs/ and data/raw/txts/\")\n",
    "\n",
    "# List available files\n",
    "print(\"\\nğŸ“ Available data files:\")\n",
    "!find data -type f 2>/dev/null | sort || echo \"No data files found\"\n",
    "\n",
    "print(\"\\nâœ… Data upload complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-pipeline"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 6. RUN COMPLETE PIPELINE\n",
    "# =======================\n",
    "print(\"ğŸš€ Running Complete Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if run script exists\n",
    "run_script = workspace / \"run_pipeline.py\"\n",
    "if run_script.exists():\n",
    "    print(\"\\nğŸ”„ Executing run_pipeline.py...\")\n",
    "    !python run_pipeline.py\n",
    "else:\n",
    "    print(\"\\nâš ï¸ run_pipeline.py not found\")\n",
    "    print(\"Running scripts directly...\")\n",
    "    \n",
    "    scripts = [\n",
    "        \"scripts/parsing_codes/statim_parser.py\",\n",
    "        \"scripts/parsing_codes/ritter_pdf_to_txt.py\",\n",
    "        \"scripts/parsing_codes/ritter_txt_to_csv.py\",\n",
    "        \"scripts/statistics_codes/merged2.py\",\n",
    "        \"scripts/statistics_codes/report2.py\"\n",
    "    ]\n",
    "    \n",
    "    for script in scripts:\n",
    "        if os.path.exists(script):\n",
    "            print(f\"\\nâ–¶ï¸  Running: {script}\")\n",
    "            !python {script}\n",
    "        else:\n",
    "            print(f\"\\nâ¸ï¸  Skipping (not found): {script}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Pipeline execution attempted\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-results"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 7. CHECK RESULTS\n",
    "# =======================\n",
    "print(\"ğŸ“Š Pipeline Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“ Generated files:\")\n",
    "!find . -type f \\( -name \"*.csv\" -o -name \"*.pdf\" -o -name \"*.txt\" -o -name \"*.png\" -o -name \"*.json\" -o -name \"*.pkl\" \\) 2>/dev/null | sort | head -20\n",
    "\n",
    "print(\"\\nğŸ“‚ Output directory:\")\n",
    "!ls -la output/ 2>/dev/null || echo \"Output directory not found\"\n",
    "\n",
    "print(\"\\nğŸ“ˆ File counts:\")\n",
    "!echo \"PDFs: $(find . -name '*.pdf' -type f 2>/dev/null | wc -l)\"\n",
    "!echo \"CSVs: $(find . -name '*.csv' -type f 2>/dev/null | wc -l)\"\n",
    "\n",
    "print(\"\\nâœ… Results check complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 8. DOWNLOAD RESULTS\n",
    "# =======================\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Download Options\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Find PDF reports\n",
    "    import glob\n",
    "    pdf_reports = glob.glob(\"**/*.pdf\", recursive=True)\n",
    "    \n",
    "    if pdf_reports:\n",
    "        print(f\"\\nğŸ“„ Found {len(pdf_reports)} PDF report(s):\")\n",
    "        for pdf in pdf_reports:\n",
    "            size_mb = os.path.getsize(pdf) / (1024 * 1024)\n",
    "            print(f\"  â€¢ {pdf} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        print(\"\\nâ¬‡ï¸  Downloading PDF reports...\")\n",
    "        for pdf in pdf_reports[:3]:  # First 3\n",
    "            try:\n",
    "                files.download(pdf)\n",
    "                print(f\"  âœ… Downloaded: {os.path.basename(pdf)}\")\n",
    "            except:\n",
    "                print(f\"  âš ï¸  Could not download: {os.path.basename(pdf)}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No PDF reports generated\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ To download all outputs:\")\n",
    "    print(\"!cd /content/medical_pipeline && zip -r results.zip output/\")\n",
    "    print(\"files.download('/content/medical_pipeline/results.zip')\")\n",
    "\n",
    "print(\"\\nğŸ‰ Pipeline Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "# ğŸ‰ Medical Data Pipeline - Complete!\n",
    "\n",
    "## âœ… What We Built:\n",
    "1. **Automated setup** - Downloads scripts from GitHub\n",
    "2. **Dependency management** - Installs all required packages\n",
    "3. **File organization** - Proper directory structure\n",
    "4. **Pipeline execution** - Runs all 5 scripts in order\n",
    "5. **Result handling** - Generates and downloads outputs\n",
    "\n",
    "## ğŸ“ Repository Structure:\n",
    "```\n",
    "medical-pipeline-colab/\n",
    "â”œâ”€â”€ Medical_Data_Pipeline.ipynb    # This notebook\n",
    "â”œâ”€â”€ setup_colab.py                 # Environment setup\n",
    "â”œâ”€â”€ run_pipeline.py                # Pipeline runner\n",
    "â”œâ”€â”€ requirements.txt               # Dependencies\n",
    "â”œâ”€â”€ README.md                      # Documentation\n",
    "â””â”€â”€ .gitignore                     # Git ignore rules\n",
    "```\n",
    "\n",
    "## ğŸ”— Links:\n",
    "- **GitHub**: https://github.com/Matlubenya/medical-pipeline-colab\n",
    "- **Colab**: https://colab.research.google.com/github/Matlubenya/medical-pipeline-colab/blob/main/Medical_Data_Pipeline.ipynb\n",
    "\n",
    "## ğŸš€ Next Steps:\n",
    "1. Test with your actual scripts and data\n",
    "2. Adjust `setup_colab.py` for any hardcoded paths\n",
    "3. Share the Colab link with colleagues\n",
    "4. Add sample data to the repository\n",
    "\n",
    "**Happy analyzing!** ğŸ¥ğŸ“Š"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
