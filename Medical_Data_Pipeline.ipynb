{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {"id": "title"},
   "source": [
    "# ğŸ¥ Ritter & STATIM Data Pipeline\n",
    "\n",
    "**Processes:**\n",
    "1. Ritter PDF â†’ TXT â†’ CSV\n",
    "2. STATIM TXT â†’ CSV  \n",
    "3. Statistical Analysis\n",
    "4. PDF Report Generation\n",
    "\n",
    "**Files needed:**\n",
    "- 2 Ritter PDFs\n",
    "- 2 STATIM TXT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "setup"},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. COLAB SETUP\n",
    "# ======================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Detect Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"ğŸš€ Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive for large files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Use temp workspace\n",
    "    workspace = Path('/content/ritter_statim_pipeline')\n",
    "    if workspace.exists():\n",
    "        shutil.rmtree(workspace)\n",
    "    workspace.mkdir()\n",
    "    os.chdir(workspace)\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    workspace = Path.cwd()\n",
    "    print(\"ğŸ’» Running locally\")\n",
    "\n",
    "print(f\"ğŸ“ Workspace: {workspace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "install"},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 2. INSTALL DEPENDENCIES\n",
    "# ======================\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Installing dependencies...\")\n",
    "    !pip install -q pandas numpy matplotlib seaborn PyPDF2 reportlab\n",
    "    # Optional: check if you use pdfplumber instead\n",
    "    # !pip install -q pdfplumber\n",
    "    print(\"âœ… Dependencies installed\")\n",
    "else:\n",
    "    print(\"Skipping install (assuming local env)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "upload-scripts"},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 3. UPLOAD/LOAD SCRIPTS\n",
    "# ======================\n",
    "script_files = [\n",
    "    \"ritter_pdf_to_txt.py\",\n",
    "    \"ritter_txt_to_csv.py\",\n",
    "    \"statim_parser.py\",\n",
    "    \"merged2.py\",\n",
    "    \"report2.py\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Loading pipeline scripts...\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Method 1: Upload via Colab interface\n",
    "    print(\"\\nPlease upload these 5 Python files:\")\n",
    "    for i, script in enumerate(script_files, 1):\n",
    "        print(f\"  {i}. {script}\")\n",
    "    print(\"\\nUse the file browser on the left â†’ Upload button\")\n",
    "    \n",
    "    # Wait for upload\n",
    "    import time\n",
    "    uploaded_all = False\n",
    "    for _ in range(30):  # Wait 30 seconds\n",
    "        if all(Path(f).exists() for f in script_files):\n",
    "            uploaded_all = True\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    \n",
    "    if uploaded_all:\n",
    "        print(\"âœ… All scripts uploaded\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Some scripts missing. Continuing anyway...\")\n",
    "else:\n",
    "    # Method 2: Copy from local directory\n",
    "    source_dir = Path(\"~/github_version/scripts\").expanduser()\n",
    "    \n",
    "    # Copy scripts\n",
    "    for script in script_files:\n",
    "        if \"ritter\" in script:\n",
    "            src = source_dir / \"parsing_codes\" / script\n",
    "        else:\n",
    "            src = source_dir / \"statistics_codes\" / script\n",
    "        \n",
    "        if src.exists():\n",
    "            shutil.copy(src, script)\n",
    "            print(f\"  âœ… {script}\")\n",
    "        else:\n",
    "            print(f\"  âŒ {script} not found at {src}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "upload-data"},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 4. UPLOAD DATA FILES\n",
    "# ======================\n",
    "# Create data directories\n",
    "raw_dir = workspace / \"raw_data\"\n",
    "raw_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ Data directory structure:\")\n",
    "print(f\"  {raw_dir}/\")\n",
    "print(\"    â”œâ”€â”€ ritter/      (for PDF files)\")\n",
    "print(\"    â””â”€â”€ statim/      (for TXT files)\")\n",
    "\n",
    "ritter_dir = raw_dir / \"ritter\"\n",
    "statim_dir = raw_dir / \"statim\"\n",
    "ritter_dir.mkdir(exist_ok=True)\n",
    "statim_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nğŸ“¤ Upload your data files:\")\n",
    "    print(\"1. Upload Ritter PDFs to 'ritter/' folder\")\n",
    "    print(\"2. Upload STATIM TXTs to 'statim/' folder\")\n",
    "    print(\"\\nUse the file browser on the left\")\n",
    "    \n",
    "    # Give time for manual upload\n",
    "    import time\n",
    "    print(\"\\nWaiting 60 seconds for file upload...\")\n",
    "    time.sleep(60)\n",
    "    \n",
    "    # Check what was uploaded\n",
    "    ritter_pdfs = list(ritter_dir.glob(\"*.pdf\"))\n",
    "    statim_txts = list(statim_dir.glob(\"*.txt\"))\n",
    "    \n",
    "    print(f\"\\nFound {len(ritter_pdfs)} Ritter PDFs: {[p.name for p in ritter_pdfs]}\")\n",
    "    print(f\"Found {len(statim_txts)} STATIM TXTs: {[t.name for t in statim_txts]}\")\n",
    "    \n",
    "    if len(ritter_pdfs) >= 2 and len(statim_txts) >= 2:\n",
    "        print(\"âœ… Data ready!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Need at least 2 PDFs and 2 TXTs\")\n",
    "        print(\"You can also use Google Drive paths:\")\n",
    "        print(\"  ritter_pdf_1 = '/content/drive/MyDrive/your_data/ritter1.pdf'\")\n",
    "        print(\"  etc.\")\n",
    "else:\n",
    "    print(\"\\nUsing local data files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "import-scripts"},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 5. IMPORT SCRIPTS\n",
    "# ======================\n",
    "print(\"ğŸ”§ Importing pipeline modules...\")\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.insert(0, str(workspace))\n",
    "\n",
    "try:\n",
    "    # Import each module\n",
    "    import importlib\n",
    "    \n",
    "    # Import Ritter PDF parser\n",
    "    spec1 = importlib.util.spec_from_file_location(\"ritter_pdf_parser\", \"ritter_pdf_to_txt.py\")\n",
    "    ritter_pdf = importlib.util.module_from_spec(spec1)\n",
    "    spec1.loader.exec_module(ritter_pdf)\n",
    "    \n",
    "    # Import Ritter TXT to CSV\n",
    "    spec2 = importlib.util.spec_from_file_location(\"ritter_csv_parser\", \"ritter_txt_to_csv.py\")\n",
    "    ritter_csv = importlib.util.module_from_spec(spec2)\n",
    "    spec2.loader.exec_module(ritter_csv)\n",
    "    \n",
    "    # Import STATIM parser\n",
    "    spec3 = importlib.util.spec_from_file_location(\"statim_parser\", \"statim_parser.py\")\n",
    "    statim = importlib.util.module_from_spec(spec3)\n",
    "    spec3.loader.exec_module(statim)\n",
    "    \n",
    "    # Import analysis\n",
    "    spec4 = importlib.util.spec_from_file_location(\"merged_analysis\", \"merged2.py\")\n",
    "    analysis = importlib.util.module_from_spec(spec4)\n",
    "    spec4.loader.exec_module(analysis)\n",
    "    \n",
    "    # Import report generator\n",
    "    spec5 = importlib.util.spec_from_file_location(\"report_generator\", \"report2.py\")\n",
    "    report = importlib.util.module_from_spec(spec5)\n",
    "    spec5.loader.exec_module(report)\n",
    "    \n",
    "    print(\"âœ… All scripts imported successfully\")\n",
    "    \n",
    "    # Create aliases for easier access\n",
    "    parse_ritter_pdf = ritter_pdf.main if hasattr(ritter_pdf, 'main') else ritter_pdf.parse_pdf\n",
    "    parse_ritter_csv = ritter_csv.main if hasattr(ritter_csv, 'main') else ritter_csv.parse_txt\n",
    "    parse_statim = statim.main if hasattr(statim, 'main') else statim.parse_statim\n",
    "    run_analysis = analysis.main if hasattr(analysis, 'main') else analysis.analyze\n",
    "    generate_report = report.main if hasattr(report, 'main') else report.create_report\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "run-pipeline"},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 6. RUN PIPELINE\n",
    "# ======================\n",
    "print(\"ğŸš€ Starting Ritter/STATIM Pipeline...\")\n",
    "\n",
    "# Create output directories\n",
    "output_dir = workspace / \"output\"\n",
    "intermediate_dir = output_dir / \"intermediate\"\n",
    "csv_dir = output_dir / \"csv\"\n",
    "results_dir = output_dir / \"results\"\n",
    "reports_dir = output_dir / \"reports\"\n",
    "\n",
    "for dir_path in [output_dir, intermediate_dir, csv_dir, results_dir, reports_dir]:\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Get data files\n",
    "ritter_pdfs = sorted(list(ritter_dir.glob(\"*.pdf\")))[:2]  # Take first 2\n",
    "statim_txts = sorted(list(statim_dir.glob(\"*.txt\")))[:2]  # Take first 2\n",
    "\n",
    "if not ritter_pdfs or not statim_txts:\n",
    "    print(\"âŒ Data files not found. Please upload files to:\")\n",
    "    print(f\"   Ritter PDFs: {ritter_dir}/\")\n",
    "    print(f\"   STATIM TXTs: {statim_dir}/\")\n",
    "else:\n",
    "    print(f\"ğŸ“„ Processing {len(ritter_pdfs)} Ritter PDFs\")\n",
    "    print(f\"ğŸ“ Processing {len(statim_txts)} STATIM TXTs\")\n",
    "    \n",
    "    try:\n",
    "        # STEP 1: Parse Ritter PDFs to TXT\n",
    "        print(\"\\n1ï¸âƒ£  Parsing Ritter PDFs to text...\")\n",
    "        ritter_txt_files = []\n",
    "        for pdf_path in ritter_pdfs:\n",
    "            txt_path = intermediate_dir / f\"{pdf_path.stem}.txt\"\n",
    "            print(f\"   Processing {pdf_path.name}...\")\n",
    "            # Call your function - adjust based on actual function name\n",
    "            parse_ritter_pdf(str(pdf_path), str(txt_path))\n",
    "            ritter_txt_files.append(txt_path)\n",
    "        print(f\"âœ… Created {len(ritter_txt_files)} text files\")\n",
    "        \n",
    "        # STEP 2: Convert Ritter TXT to CSV\n",
    "        print(\"\\n2ï¸âƒ£  Converting Ritter text to CSV...\")\n",
    "        ritter_csv_files = []\n",
    "        for txt_path in ritter_txt_files:\n",
    "            csv_path = csv_dir / f\"ritter_{txt_path.stem}.csv\"\n",
    "            print(f\"   Processing {txt_path.name}...\")\n",
    "            parse_ritter_csv(str(txt_path), str(csv_path))\n",
    "            ritter_csv_files.append(csv_path)\n",
    "        print(f\"âœ… Created {len(ritter_csv_files)} CSV files\")\n",
    "        \n",
    "        # STEP 3: Parse STATIM TXT to CSV\n",
    "        print(\"\\n3ï¸âƒ£  Parsing STATIM text to CSV...\")\n",
    "        statim_csv_files = []\n",
    "        for txt_path in statim_txts:\n",
    "            csv_path = csv_dir / f\"statim_{txt_path.stem}.csv\"\n",
    "            print(f\"   Processing {txt_path.name}...\")\n",
    "            parse_statim(str(txt_path), str(csv_path))\n",
    "            statim_csv_files.append(csv_path)\n",
    "        print(f\"âœ… Created {len(statim_csv_files)} STATIM CSV files\")\n",
    "        \n",
    "        # STEP 4: Run analysis\n",
    "        print(\"\\n4ï¸âƒ£  Running statistical analysis...\")\n",
    "        all_csv_files = ritter_csv_files + statim_csv_files\n",
    "        results = run_analysis(all_csv_files, str(results_dir))\n",
    "        print(\"âœ… Analysis complete\")\n",
    "        \n",
    "        # STEP 5: Generate report\n",
    "        print(\"\\n5ï¸âƒ£  Generating PDF report...\")\n",
    "        report_path = reports_dir / \"ritter_statim_analysis_report.pdf\"\n",
    "        generate_report(results, str(report_path))\n",
    "        \n",
    "        print(f\"\\nğŸ‰ PIPELINE COMPLETE!\")\n",
    "        print(f\"ğŸ“Š Results: {results_dir}/\")\n",
    "        print(f\"ğŸ“„ Report: {report_path}\")\n",
    "        \n",
    "        # Show generated files\n",
    "        print(\"\\nğŸ“ Generated files:\")\n",
    "        !find {output_dir} -type f -name \"*.csv\" -o -name \"*.pdf\" -o -name \"*.png\" | head -20\n",
    "        \n",
    "        # Download report in Colab\n",
    "        if IN_COLAB and report_path.exists():\n",
    "            print(\"\\nâ¬‡ï¸  Downloading report...\")\n",
    "            from google.colab import files\n",
    "            files.download(str(report_path))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Running in Google Colab\")\n",
    "    \n",
    "    # Create fresh workspace\n",
    "    workspace = Path('/content/medical_pipeline')\n",
    "    if workspace.exists():\n",
    "        shutil.rmtree(workspace)\n",
    "        print(\"ğŸ§¹ Cleaned old workspace\")\n",
    "    \n",
    "    workspace.mkdir()\n",
    "    os.chdir(workspace)\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    workspace = Path.cwd()\n",
    "    print(\"ğŸ’» Running locally\")\n",
    "\n",
    "print(f\"ğŸ“ Workspace: {workspace}\")\n",
    "\n",
    "# DOWNLOAD ESSENTIAL SCRIPTS FROM GITHUB\n",
    "if IN_COLAB:\n",
    "    print(\"\\nğŸ“¥ Downloading setup scripts from GitHub...\")\n",
    "    \n",
    "    import urllib.request\n",
    "    \n",
    "    github_repo = \"Matlubenya/medical-pipeline-colab\"\n",
    "    scripts = [\"setup_colab.py\", \"run_pipeline.py\", \"requirements.txt\"]\n",
    "    \n",
    "    for script in scripts:\n",
    "        try:\n",
    "            url = f\"https://raw.githubusercontent.com/{github_repo}/main/{script}\"\n",
    "            destination = workspace / script\n",
    "            urllib.request.urlretrieve(url, destination)\n",
    "            print(f\"  âœ… {script}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸  {script}: {e}\")\n",
    "            # Create minimal version if download fails\n",
    "            if script == \"setup_colab.py\":\n",
    "                minimal = '''import os\\nfrom pathlib import Path\\n\\ndef main():\\n    print(\\\"Setting up directory structure...\\\")\\n    workspace = Path.cwd()\\n    dirs = [\\\"scripts/parsing_codes\\\", \\\"scripts/statistics_codes\\\", \\\"data/raw/pdfs\\\", \\\"data/raw/txts\\\", \\\"output\\\"]\\n    for d in dirs:\\n        (workspace / d).mkdir(parents=True, exist_ok=True)\\n    print(\\\"âœ… Setup complete\\\")\\n    return workspace\\n\\nif __name__ == \\\"__main__\\\":\\n    main()'''\n",
    "                destination.write_text(minimal)\n",
    "            elif script == \"run_pipeline.py\":\n",
    "                minimal = '''import subprocess\\nfrom pathlib import Path\\n\\ndef main():\\n    print(\\\"Running pipeline scripts...\\\")\\n    scripts = [\\n        (\\\"scripts/parsing_codes/statim_parser.py\\\", \\\"STATIM Parser\\\"),\\n        (\\\"scripts/parsing_codes/ritter_pdf_to_txt.py\\\", \\\"Ritter PDF Parser\\\"),\\n        (\\\"scripts/parsing_codes/ritter_txt_to_csv.py\\\", \\\"Ritter CSV Converter\\\"),\\n        (\\\"scripts/statistics_codes/merged2.py\\\", \\\"Statistical Analysis\\\"),\\n        (\\\"scripts/statistics_codes/report2.py\\\", \\\"Report Generator\\\")\\n    ]\\n    \\n    for script_path, description in scripts:\\n        full_path = Path(script_path)\\n        if full_path.exists():\\n            print(f\\\"\\\\nğŸš€ {description}: {full_path.name}\\\")\\n            subprocess.run([\\\"python\\\", str(full_path)])\\n        else:\\n            print(f\\\"âš ï¸  Missing: {script_path}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()'''\n",
    "                destination.write_text(minimal)\n",
    "    \n",
    "    print(\"\\nâœ… Setup scripts downloaded\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 2. INSTALL DEPENDENCIES\n",
    "# =======================\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Installing dependencies...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Install from requirements.txt if available\n",
    "    req_file = workspace / \"requirements.txt\"\n",
    "    if req_file.exists():\n",
    "        print(\"Installing from requirements.txt...\")\n",
    "        !pip install -q -r requirements.txt\n",
    "    else:\n",
    "        # Install essential packages\n",
    "        packages = [\"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"scipy\", \"reportlab\", \"tqdm\"]\n",
    "        for pkg in packages:\n",
    "            !pip install -q {pkg}\n",
    "    \n",
    "    # Install pdftotext\n",
    "    print(\"Installing pdftotext (for PDF parsing)...\")\n",
    "    !apt-get update -qq && apt-get install -y poppler-utils 2>/dev/null\n",
    "    \n",
    "    print(\"\\nâœ… Dependencies installed\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"ğŸ’» Using local Python environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-environment"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 3. SETUP ENVIRONMENT\n",
    "# =======================\n",
    "print(\"ğŸ”§ Setting up environment...\")\n",
    "\n",
    "# Run setup script\n",
    "setup_script = workspace / \"setup_colab.py\"\n",
    "if setup_script.exists():\n",
    "    print(\"Running setup_colab.py...\")\n",
    "    !python setup_colab.py\n",
    "else:\n",
    "    print(\"âš ï¸ setup_colab.py not found. Creating basic structure...\")\n",
    "    !mkdir -p scripts/parsing_codes scripts/statistics_codes data/raw/pdfs data/raw/txts output\n",
    "\n",
    "print(\"\\nğŸ“ Directory structure:\")\n",
    "!find . -maxdepth 3 -type d | sort\n",
    "\n",
    "print(\"\\nâœ… Environment ready\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-scripts"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 4. UPLOAD PIPELINE SCRIPTS\n",
    "# =======================\n",
    "print(\"ğŸ“ Upload Your 5 Pipeline Scripts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"\\nğŸ“¤ Please upload these 5 Python files:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    scripts_needed = [\n",
    "        (\"ritter_pdf_to_txt.py\", \"parsing_codes\"),\n",
    "        (\"ritter_txt_to_csv.py\", \"parsing_codes\"),\n",
    "        (\"statim_parser.py\", \"parsing_codes\"),\n",
    "        (\"merged2.py\", \"statistics_codes\"),\n",
    "        (\"report2.py\", \"statistics_codes\")\n",
    "    ]\n",
    "    \n",
    "    for script, folder in scripts_needed:\n",
    "        print(f\"  â€¢ {script} â†’ scripts/{folder}/\")\n",
    "    \n",
    "    print(\"\\nâ„¹ï¸  Use the file browser on the left\")\n",
    "    print(\"   Upload one by one or all at once\")\n",
    "    \n",
    "    # Wait for upload\n",
    "    input(\"\\nğŸ“ Press Enter AFTER uploading all 5 scripts...\")\n",
    "    \n",
    "    # Organize uploaded files\n",
    "    import glob\n",
    "    uploaded = glob.glob(\"*.py\")\n",
    "    \n",
    "    if uploaded:\n",
    "        print(f\"\\nğŸ“¦ Organizing {len(uploaded)} uploaded files...\")\n",
    "        \n",
    "        for script_path in uploaded:\n",
    "            script_name = os.path.basename(script_path)\n",
    "            target_folder = None\n",
    "            \n",
    "            # Find correct folder\n",
    "            for needed_script, folder in scripts_needed:\n",
    "                if script_name == needed_script:\n",
    "                    target_folder = folder\n",
    "                    break\n",
    "            \n",
    "            if target_folder:\n",
    "                target_path = f\"scripts/{target_folder}/{script_name}\"\n",
    "                os.rename(script_path, target_path)\n",
    "                size_kb = os.path.getsize(target_path) / 1024\n",
    "                print(f\"  âœ… {script_name} â†’ scripts/{target_folder}/ ({size_kb:.1f} KB)\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  {script_name} (not a required script)\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No Python files uploaded\")\n",
    "        print(\"   You can upload later using file browser\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nğŸ’» Local mode: Ensure scripts are in scripts/parsing_codes/ and scripts/statistics_codes/\")\n",
    "\n",
    "# Verify scripts\n",
    "print(\"\\nğŸ“‹ Script Status:\")\n",
    "missing = []\n",
    "for script, folder in scripts_needed:\n",
    "    script_path = workspace / \"scripts\" / folder / script\n",
    "    if script_path.exists():\n",
    "        print(f\"  âœ… {script}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {script} (MISSING)\")\n",
    "        missing.append(script)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nâš ï¸  Missing {len(missing)} scripts\")\n",
    "    print(\"   Upload them using the file browser\")\n",
    "else:\n",
    "    print(\"\\nğŸ‰ All pipeline scripts ready!\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-data"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 5. UPLOAD DATA FILES\n",
    "# =======================\n",
    "print(\"ğŸ“¤ Upload Data Files\")\n,
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"\\nğŸ“„ Upload 2 Ritter PDF files:\")\n",
    "    print(\"Files will be saved to: data/raw/pdfs/\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    pdf_files = []\n",
    "    \n",
    "    for filename, content in uploaded.items():\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            dest = workspace / \"data\" / \"raw\" / \"pdfs\" / filename\n",
    "            dest.write_bytes(content)\n",
    "            pdf_files.append(filename)\n",
    "            size_mb = len(content) / (1024 * 1024)\n",
    "            print(f\"  âœ… {filename} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Upload 2 STATIM TXT files:\")\n",
    "    print(\"Files will be saved to: data/raw/txts/\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    uploaded = files.upload()\n",
    "    txt_files = []\n",
    "    \n",
    "    for filename, content in uploaded.items():\n",
    "        if filename.lower().endswith('.txt'):\n",
    "            dest = workspace / \"data\" / \"raw\" / \"txts\" / filename\n",
    "            dest.write_bytes(content)\n",
    "            txt_files.append(filename)\n",
    "            size_kb = len(content) / 1024\n",
    "            print(f\"  âœ… {filename} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Upload Summary:\")\n",
    "    print(f\"  â€¢ PDF files: {len(pdf_files)} ({', '.join(pdf_files[:3])}{'...' if len(pdf_files) > 3 else ''})\")\n",
    "    print(f\"  â€¢ TXT files: {len(txt_files)} ({', '.join(txt_files[:3])}{'...' if len(txt_files) > 3 else ''})\")\n",
    "    \n",
    "    if len(pdf_files) < 1 or len(txt_files) < 1:\n",
    "        print(\"\\nâš ï¸  Need at least 1 PDF and 1 TXT file\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nğŸ’» Local mode: Ensure data files are in data/raw/pdfs/ and data/raw/txts/\")\n",
    "\n",
    "# List available files\n",
    "print(\"\\nğŸ“ Available data files:\")\n",
    "!find data -type f 2>/dev/null | sort || echo \"No data files found\"\n",
    "\n",
    "print(\"\\nâœ… Data upload complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-pipeline"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 6. RUN COMPLETE PIPELINE\n",
    "# =======================\n",
    "print(\"ğŸš€ Running Complete Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if run script exists\n",
    "run_script = workspace / \"run_pipeline.py\"\n",
    "if run_script.exists():\n",
    "    print(\"\\nğŸ”„ Executing run_pipeline.py...\")\n",
    "    !python run_pipeline.py\n",
    "else:\n",
    "    print(\"\\nâš ï¸ run_pipeline.py not found\")\n",
    "    print(\"Running scripts directly...\")\n",
    "    \n",
    "    scripts = [\n",
    "        \"scripts/parsing_codes/statim_parser.py\",\n",
    "        \"scripts/parsing_codes/ritter_pdf_to_txt.py\",\n",
    "        \"scripts/parsing_codes/ritter_txt_to_csv.py\",\n",
    "        \"scripts/statistics_codes/merged2.py\",\n",
    "        \"scripts/statistics_codes/report2.py\"\n",
    "    ]\n",
    "    \n",
    "    for script in scripts:\n",
    "        if os.path.exists(script):\n",
    "            print(f\"\\nâ–¶ï¸  Running: {script}\")\n",
    "            !python {script}\n",
    "        else:\n",
    "            print(f\"\\nâ¸ï¸  Skipping (not found): {script}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Pipeline execution attempted\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-results"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 7. CHECK RESULTS\n",
    "# =======================\n",
    "print(\"ğŸ“Š Pipeline Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ“ Generated files:\")\n",
    "!find . -type f \\( -name \"*.csv\" -o -name \"*.pdf\" -o -name \"*.txt\" -o -name \"*.png\" -o -name \"*.json\" -o -name \"*.pkl\" \\) 2>/dev/null | sort | head -20\n",
    "\n",
    "print(\"\\nğŸ“‚ Output directory:\")\n",
    "!ls -la output/ 2>/dev/null || echo \"Output directory not found\"\n",
    "\n",
    "print(\"\\nğŸ“ˆ File counts:\")\n",
    "!echo \"PDFs: $(find . -name '*.pdf' -type f 2>/dev/null | wc -l)\"\n",
    "!echo \"CSVs: $(find . -name '*.csv' -type f 2>/dev/null | wc -l)\"\n",
    "\n",
    "print(\"\\nâœ… Results check complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# 8. DOWNLOAD RESULTS\n",
    "# =======================\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Download Options\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    # Find PDF reports\n",
    "    import glob\n",
    "    pdf_reports = glob.glob(\"**/*.pdf\", recursive=True)\n",
    "    \n",
    "    if pdf_reports:\n",
    "        print(f\"\\nğŸ“„ Found {len(pdf_reports)} PDF report(s):\")\n",
    "        for pdf in pdf_reports:\n",
    "            size_mb = os.path.getsize(pdf) / (1024 * 1024)\n",
    "            print(f\"  â€¢ {pdf} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        print(\"\\nâ¬‡ï¸  Downloading PDF reports...\")\n",
    "        for pdf in pdf_reports[:3]:  # First 3\n",
    "            try:\n",
    "                files.download(pdf)\n",
    "                print(f\"  âœ… Downloaded: {os.path.basename(pdf)}\")\n",
    "            except:\n",
    "                print(f\"  âš ï¸  Could not download: {os.path.basename(pdf)}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No PDF reports generated\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ To download all outputs:\")\n",
    "    print(\"!cd /content/medical_pipeline && zip -r results.zip output/\")\n",
    "    print(\"files.download('/content/medical_pipeline/results.zip')\")\n",
    "\n",
    "print(\"\\nğŸ‰ Pipeline Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "# ğŸ‰ Medical Data Pipeline - Complete!\n",
    "\n",
    "## âœ… What We Built:\n",
    "1. **Automated setup** - Downloads scripts from GitHub\n",
    "2. **Dependency management** - Installs all required packages\n",
    "3. **File organization** - Proper directory structure\n",
    "4. **Pipeline execution** - Runs all 5 scripts in order\n",
    "5. **Result handling** - Generates and downloads outputs\n",
    "\n",
    "## ğŸ“ Repository Structure:\n",
    "```\n",
    "medical-pipeline-colab/\n",
    "â”œâ”€â”€ Medical_Data_Pipeline.ipynb    # This notebook\n",
    "â”œâ”€â”€ setup_colab.py                 # Environment setup\n",
    "â”œâ”€â”€ run_pipeline.py                # Pipeline runner\n",
    "â”œâ”€â”€ requirements.txt               # Dependencies\n",
    "â”œâ”€â”€ README.md                      # Documentation\n",
    "â””â”€â”€ .gitignore                     # Git ignore rules\n",
    "```\n",
    "\n",
    "## ğŸ”— Links:\n",
    "- **GitHub**: https://github.com/Matlubenya/medical-pipeline-colab\n",
    "- **Colab**: https://colab.research.google.com/github/Matlubenya/medical-pipeline-colab/blob/main/Medical_Data_Pipeline.ipynb\n",
    "\n",
    "## ğŸš€ Next Steps:\n",
    "1. Test with your actual scripts and data\n",
    "2. Adjust `setup_colab.py` for any hardcoded paths\n",
    "3. Share the Colab link with colleagues\n",
    "4. Add sample data to the repository\n",
    "\n",
    "**Happy analyzing!** ğŸ¥ğŸ“Š"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
